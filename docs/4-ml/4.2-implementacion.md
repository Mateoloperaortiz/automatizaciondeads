# 4.2 Implementación del Componente ML

## Estructura del Módulo

El componente de aprendizaje automático está implementado en el archivo `adflux/ml_model.py`. Este módulo contiene todas las funciones necesarias para el preprocesamiento de datos, entrenamiento del modelo, predicción de segmentos y análisis de resultados.

## Importaciones y Configuración

```python
import os
import joblib
import numpy as np
import pandas as pd
from typing import Tuple, Optional, Dict, List, Any
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import silhouette_score, davies_bouldin_score

# Configuración por defecto
DEFAULT_N_CLUSTERS = 5
DEFAULT_MODEL_PATH = 'instance/ml_models/kmeans_model.joblib'
DEFAULT_PREPROCESSOR_PATH = 'instance/ml_models/preprocessor.joblib'
```

## Preprocesamiento de Datos

El preprocesamiento es un paso crucial para preparar los datos de candidatos para el algoritmo K-means. Se implementa mediante un `ColumnTransformer` que aplica diferentes transformaciones a diferentes tipos de columnas.

```python
def create_preprocessor() -> ColumnTransformer:
    """Crea el ColumnTransformer para preprocesar datos de candidatos."""
    
    # Definir transformadores para diferentes tipos de columnas
    numerical_features = ['years_experience', 'desired_salary']
    categorical_features = ['location', 'education_level', 'primary_skill']
    text_features = 'skills_text'
    
    # Pipeline para características numéricas
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    # Pipeline para características categóricas
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    
    # Pipeline para características de texto (skills)
    text_transformer = TfidfVectorizer(stop_words='english', max_features=100)
    
    # Crear el ColumnTransformer
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features),
            ('text', text_transformer, text_features)
        ],
        remainder='drop'
    )
    
    return preprocessor
```

### Detalles del Preprocesamiento

1. **Características Numéricas**:
   - **Imputación**: Reemplaza valores faltantes con la mediana
   - **Escalado**: Estandariza las características para tener media 0 y desviación estándar 1

2. **Características Categóricas**:
   - **Imputación**: Reemplaza valores faltantes con 'Unknown'
   - **Codificación One-Hot**: Convierte categorías en vectores binarios

3. **Características Textuales**:
   - **Vectorización TF-IDF**: Convierte texto de habilidades en vectores numéricos
   - **Limitación de Características**: Usa solo las 100 características más importantes

## Entrenamiento del Modelo

La función de entrenamiento toma un DataFrame de candidatos, aplica el preprocesamiento y entrena un modelo K-means.

```python
def train_segmentation_model(
    df: pd.DataFrame, 
    n_clusters: int = DEFAULT_N_CLUSTERS,
    model_path: str = DEFAULT_MODEL_PATH, 
    preprocessor_path: str = DEFAULT_PREPROCESSOR_PATH
) -> Tuple[KMeans, ColumnTransformer]:
    """
    Preprocesa datos de candidatos, entrena un modelo K-means y guarda ambos.
    
    Args:
        df: DataFrame con datos de candidatos
        n_clusters: Número de clústeres a crear
        model_path: Ruta donde guardar el modelo entrenado
        preprocessor_path: Ruta donde guardar el preprocesador
        
    Returns:
        Tupla con el modelo K-means entrenado y el preprocesador
    """
    if df.empty:
        raise ValueError("No se puede entrenar el modelo en un DataFrame vacío")
        
    # Asegurar que 'skills_text' exista
    if 'skills_text' not in df.columns and 'skills' in df.columns:
        df['skills_text'] = df['skills'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')
    
    # Crear y ajustar el preprocesador
    preprocessor = create_preprocessor()
    X_processed = preprocessor.fit_transform(df)
    
    # Entrenar el modelo K-means
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    kmeans.fit(X_processed)
    
    # Calcular métricas de evaluación
    labels = kmeans.labels_
    metrics = {}
    
    if len(set(labels)) > 1:  # Solo calcular si hay más de un clúster
        metrics['inertia'] = kmeans.inertia_
        try:
            metrics['silhouette_score'] = silhouette_score(X_processed, labels)
            metrics['davies_bouldin_score'] = davies_bouldin_score(X_processed, labels)
        except:
            # Algunas métricas pueden fallar con ciertos datos
            pass
    
    # Guardar el modelo y el preprocesador
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    os.makedirs(os.path.dirname(preprocessor_path), exist_ok=True)
    joblib.dump(kmeans, model_path)
    joblib.dump(preprocessor, preprocessor_path)
    
    # Guardar métricas
    metrics_path = os.path.join(os.path.dirname(model_path), 'metrics.joblib')
    joblib.dump(metrics, metrics_path)
    
    return kmeans, preprocessor
```

### Detalles del Entrenamiento

- **Preparación de Datos**: Asegura que exista la columna 'skills_text' para la vectorización TF-IDF
- **Ajuste del Preprocesador**: Aprende los parámetros de transformación de los datos
- **Entrenamiento K-means**: Utiliza el algoritmo K-means con inicialización múltiple
- **Evaluación**: Calcula métricas como inercia, coeficiente de silueta e índice Davies-Bouldin
- **Persistencia**: Guarda el modelo, el preprocesador y las métricas para uso futuro

## Carga del Modelo

```python
def load_segmentation_model(
    model_path: str = DEFAULT_MODEL_PATH,
    preprocessor_path: str = DEFAULT_PREPROCESSOR_PATH
) -> Tuple[Optional[KMeans], Optional[ColumnTransformer]]:
    """
    Carga el modelo K-means y el preprocesador desde archivos.
    
    Args:
        model_path: Ruta del modelo guardado
        preprocessor_path: Ruta del preprocesador guardado
        
    Returns:
        Tupla con el modelo K-means y el preprocesador, o (None, None) si no se encuentran
    """
    try:
        if os.path.exists(model_path) and os.path.exists(preprocessor_path):
            kmeans = joblib.load(model_path)
            preprocessor = joblib.load(preprocessor_path)
            return kmeans, preprocessor
        else:
            print(f"Archivos no encontrados: {model_path} o {preprocessor_path}")
            return None, None
    except Exception as e:
        print(f"Error al cargar el modelo o preprocesador: {e}")
        return None, None
```

## Predicción de Segmentos

```python
def predict_candidate_segments(df: pd.DataFrame, 
                               model: Optional[KMeans] = None, 
                               preprocessor: Optional[ColumnTransformer] = None) -> pd.DataFrame:
    """
    Predice segmentos de clúster para candidatos en el DataFrame.
    Carga el modelo y el preprocesador si no se proporcionan.
    
    Args:
        df: DataFrame con datos de candidatos
        model: Modelo K-means opcional
        preprocessor: Preprocesador opcional
        
    Returns:
        DataFrame con columna 'segment' añadida
    """
    if model is None or preprocessor is None:
        model, preprocessor = load_segmentation_model()
        
    if model is None or preprocessor is None:
        df['segment'] = -1
        return df

    # Asegurar que 'skills_text' exista
    if 'skills_text' not in df.columns and 'skills' in df.columns:
         df['skills_text'] = df['skills'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')
    
    # Preprocesar y predecir
    X_processed = preprocessor.transform(df)
    segments = model.predict(X_processed)
    df['segment'] = segments
    
    return df
```

## Análisis de Segmentos

```python
def analyze_segments_from_db():
    """
    Obtiene candidatos de la BD, analiza segmentos y devuelve resultados como un diccionario.
    
    Returns:
        Diccionario con análisis detallado de cada segmento
    """
    from .models import Candidate
    from flask import current_app
    
    analysis_results = {}
    candidates = Candidate.query.all()
    
    # Convertir a DataFrame
    df = pd.DataFrame([{
        'id': c.candidate_id,
        'name': c.name,
        'title': c.primary_skill,
        'skills': c.skills,
        'experience_years': c.years_experience,
        'location': c.location,
        'segment': c.segment_id
    } for c in candidates])
    
    segments = sorted(df['segment'].dropna().unique())
    
    for segment_id in segments:
        segment_df = df[df['segment'] == segment_id].copy()
        segment_data = {}
        segment_data['count'] = len(segment_df)
        
        # Estadísticas de Experiencia
        if 'experience_years' in segment_df.columns:
            stats = segment_df['experience_years'].describe()
            segment_data['experience_stats'] = {
                'mean': float(stats.get('mean', 0)),
                'std': float(stats.get('std', 0)),
                'min': float(stats.get('min', 0)),
                'max': float(stats.get('max', 0)),
                'median': float(stats.get('50%', 0))
            }
        
        # Habilidades Principales Top
        if 'title' in segment_df.columns:
            top_primary = segment_df['title'].value_counts().nlargest(5)
            segment_data['top_primary_skills'] = list(top_primary.items())
            
            # Datos para Gráfico
            chart_labels = [item[0] for item in segment_data['top_primary_skills']]
            chart_data = [item[1] for item in segment_data['top_primary_skills']]
            segment_data['primary_skills_chart'] = {
                'labels': chart_labels,
                'data': chart_data
            }
        
        # Ubicaciones Top
        if 'location' in segment_df.columns:
            top_locations = segment_df['location'].value_counts().nlargest(5)
            segment_data['locations'] = list(top_locations.items())
        
        # Datos para Gráfico de Experiencia
        if 'experience_years' in segment_df.columns:
            exp_data = segment_df['experience_years']
            bins = [0, 5, 10, 15, 20, 26]
            labels = ['0-4 yrs', '5-9 yrs', '10-14 yrs', '15-19 yrs', '20+ yrs']
            exp_binned = pd.cut(exp_data, bins=bins, labels=labels, right=False)
            exp_counts = exp_binned.value_counts().sort_index()
            segment_data['experience_chart'] = {
                'labels': list(exp_counts.index.astype(str)),
                'data': list(exp_counts.values.astype(float))
            }
        
        # Habilidades Generales Top
        if 'skills' in segment_df.columns:
            skills_series = segment_df['skills'].dropna()
            skills_series = skills_series[skills_series.apply(lambda x: isinstance(x, list))]
            if not skills_series.empty:
                exploded_skills = skills_series.explode()
                valid_skills = exploded_skills.astype(str).str.strip()
                valid_skills = valid_skills[valid_skills != '']
                if not valid_skills.empty:
                    top_overall = valid_skills.value_counts().nlargest(10)
                    segment_data['top_overall_skills'] = list(top_overall.items())
        
        analysis_results[segment_id] = segment_data
    
    # Calcular Datos de Gráfico Generales
    # ... (código para gráficos generales)
    
    return analysis_results
```

### Detalles del Análisis

El análisis de segmentos proporciona información detallada sobre cada grupo:

1. **Estadísticas Básicas**:
   - Recuento de candidatos por segmento
   - Estadísticas de experiencia (media, desviación estándar, mínimo, máximo, mediana)

2. **Habilidades**:
   - Habilidades principales más comunes
   - Habilidades generales más frecuentes

3. **Ubicaciones**:
   - Ubicaciones geográficas más comunes

4. **Distribución de Experiencia**:
   - Distribución de candidatos por rangos de años de experiencia

5. **Datos para Visualización**:
   - Estructuras de datos preparadas para gráficos y visualizaciones

## Actualización de Segmentos en la Base de Datos

```python
def update_candidate_segments_in_db():
    """
    Actualiza los segmentos de todos los candidatos en la base de datos.
    
    Returns:
        Diccionario con resultados de la operación
    """
    from .models import Candidate, db
    
    try:
        # Obtener todos los candidatos
        candidates = Candidate.query.all()
        
        if not candidates:
            return {'status': 'warning', 'message': 'No hay candidatos en la base de datos'}
        
        # Convertir a DataFrame
        df = pd.DataFrame([{
            'candidate_id': c.candidate_id,
            'name': c.name,
            'location': c.location,
            'years_experience': c.years_experience,
            'education_level': c.education_level,
            'skills': c.skills,
            'primary_skill': c.primary_skill,
            'desired_salary': c.desired_salary,
            'skills_text': ' '.join(c.skills) if isinstance(c.skills, list) else ''
        } for c in candidates])
        
        # Predecir segmentos
        df = predict_candidate_segments(df)
        
        # Actualizar segmentos en la base de datos
        updated_count = 0
        for _, row in df.iterrows():
            candidate = next((c for c in candidates if c.candidate_id == row['candidate_id']), None)
            if candidate and row['segment'] != -1:
                candidate.segment_id = int(row['segment'])
                updated_count += 1
        
        # Guardar cambios
        db.session.commit()
        
        return {
            'status': 'success',
            'message': f'Segmentos actualizados para {updated_count} candidatos',
            'updated_count': updated_count,
            'total_candidates': len(candidates)
        }
    
    except Exception as e:
        db.session.rollback()
        return {'status': 'error', 'message': f'Error al actualizar segmentos: {str(e)}'}
```

## Funciones Auxiliares

```python
def get_segment_names():
    """
    Obtiene los nombres de los segmentos desde la base de datos.
    
    Returns:
        Diccionario con IDs de segmento como claves y nombres como valores
    """
    from .models import Segment
    
    segments = Segment.query.all()
    return {s.id: s.name for s in segments}

def get_segment_distribution():
    """
    Obtiene la distribución de candidatos por segmento.
    
    Returns:
        Diccionario con datos para gráfico de distribución
    """
    from .models import Candidate, db
    from sqlalchemy import func
    
    # Contar candidatos por segmento
    segment_counts = db.session.query(
        Candidate.segment_id, 
        func.count(Candidate.candidate_id)
    ).group_by(Candidate.segment_id).all()
    
    # Obtener nombres de segmentos
    segment_names = get_segment_names()
    
    # Preparar datos para gráfico
    labels = []
    data = []
    
    for segment_id, count in segment_counts:
        if segment_id is not None:
            name = segment_names.get(segment_id, f"Segmento {segment_id}")
            labels.append(name)
            data.append(count)
    
    return {
        'labels': labels,
        'data': data
    }
```
